# -*- coding: utf-8 -*-
"""MetricOptimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1esOudg0GIHkLnH0uG8jyMVLq75leZFdB

Scikit-learn optimization

Grid Search and RandomSearch and use Cross-Validation

https://scikit-learn.org/stable/modules/classes.html?highlight=metrics#module-sklearn.metrics
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, make_scorer

#load data from scikit-learn
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y = True)

"""map() function returns a map object(which is an iterator) of the results after applying the given function to each item of a given iterable (list, tuple etc.)

Syntax :

map(fun, iter)
"""

X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1,1:0})

z=[1,0,1,1,0]
z = pd.Series(z)
#sam as z.map({0:1,1:0})
map({0:1,1:0},z)

z

X.head()

y.value_counts() / len(y)

""".ravel() function

x = np.array([[1, 2, 3], [4, 5, 6]])

np.ravel(x)

-->array([1, 2, 3, 4, 5, 6])
"""

#function returns the FNR 
def fnr(y_true, y_pred):
  tn,fp,fn,tp = confusion_matrix(y_true, y_pred,labels=[0,1]).ravel()
  FNR = fn / (tp +fn)
  return FNR

#make the criterior for scoring.
fnr_score = make_scorer(fnr, greater_is_better=False,needs_proba=False)

#Ramdom forests
rf_model = RandomForestClassifier(n_estimators=100,max_depth=1,random_state=0,n_jobs=4)

#Hyperparameter space
rf_param_grid = dict(
    n_estimators=[10,20,50,100,200,500],
    max_depth = [1,2,3,4],
)

#search
clf = GridSearchCV(rf_model,rf_param_grid,scoring=fnr_score,cv=5)

search = clf.fit(X,y)

#best hyperparameters
search.best_params_

results = pd.DataFrame(search.cv_results_)[['params','mean_test_score',"std_test_score"]]
results.head()

results.sort_values(by='mean_test_score',ascending=False,inplace=True)

results.reset_index(drop=True,inplace=True)

results['mean_test_score'].plot(yerr=[results['std_test_score'],results['std_test_score']],subplots=True)

plt.ylim=(-0.3,0)
plt.ylabel("Mean False Negative Rate")
plt.xlabel('Hyperparameter space')

"""# Scoring function with a probability threshold"""

#function 
def fnr(y_true, y_pred):

  y_pred_class = np.where(y_pred > 0.37, 1, 0)


  tn,fp,fn,tp = confusion_matrix(y_true, y_pred_class,labels=[0,1]).ravel()
  FNR = fn / (tp +fn)
  return FNR

#make the criterior for scoring.
fnr_score = make_scorer(fnr, greater_is_better=False,needs_proba=True)

#Ramdom forests
rf_model = RandomForestClassifier(n_estimators=100,max_depth=1,random_state=0,n_jobs=4)

#Hyperparameter space
rf_param_grid = dict(
    n_estimators=[10,20,50,100,200,500],
    max_depth = [1,2,3,4],
)

#search
clf = GridSearchCV(rf_model,rf_param_grid,scoring=fnr_score,cv=5)

search = clf.fit(X,y)

#best hyperparameters
search.best_params_

search.best_params_

results = pd.DataFrame(search.cv_results_)[['params','mean_test_score',"std_test_score"]]
results

results.sort_values(by='mean_test_score',ascending=False,inplace=True)

results.reset_index(drop=True,inplace=True)

results['mean_test_score'].plot(yerr=[results['std_test_score'],results['std_test_score']],subplots=True)

plt.ylim=(-0.3,0)
plt.ylabel("Mean False Negative Rate")
plt.xlabel('Hyperparameter space')

"""# ***Use Scikit-learn metrics***"""

# random forests
rf_model = RandomForestClassifier(
    n_estimators=100, max_depth=1, random_state=0, n_jobs=4)

# hyperparameter space
rf_param_grid = dict(
    n_estimators=[20, 50, 100, 200, 500, 1000],
    max_depth=[2, 3, 4],
)

# search
clf = GridSearchCV(rf_model,
                   rf_param_grid,
                   #instead create a metrics for our own, we use roc-auc from scikit-learn
                   scoring='roc_auc',
                   cv=5)

search = clf.fit(X, y)

# best hyperparameters
search.best_params_

results = pd.DataFrame(search.cv_results_)[['params','mean_test_score',"std_test_score"]]
results

results.sort_values(by='mean_test_score',ascending=False,inplace=True)

results.reset_index(drop=True,inplace=True)

results['mean_test_score'].plot(yerr=[results['std_test_score'],results['std_test_score']],subplots=True)

plt.ylim=(-0.3,0)
plt.ylabel("Mean False Negative Rate")
plt.xlabel('Hyperparameter space')



